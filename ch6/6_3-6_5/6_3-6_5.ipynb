{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 前節では「重みの初期値を適切に設定すれば，各層のアクティベーションの分布は適度な広がりを持ち，学習がスムーズに行える」ということを学んだ\n",
    "- この節ではそのアクティベーション分布を強制的に調整してみる手法 Batch Normalization を紹介する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1 Batch Normalization のアルゴリズム"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2015 年に提案された手法\n",
    "- 派生手法もたくさんあるようです\n",
    "- コンペなどで優れた結果を残している例でよく見かける（というよりほぼ必ず？）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Normalization の利点\n",
    "- 学習を速く進行させることができる（学習係数を大きくすることができる）\n",
    "- 初期値にそれほど依存しない（初期値に対してそこまで神経質にならなくてよい）\n",
    "    - この前あった「フレームワークによって同じネットワーク・初期値でもうまく収束したりしなかったりする」問題がなくなる？\n",
    "- 過学習を抑制する（Dropout などの必要性を減らす）\n",
    "    - 逆に今 Dropout って使われているんですかね？あまり見かけない気がします．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Normalization のアイデア\n",
    "- 各層でのアクティベーション分布を，適度な広がりを持つように調整すること"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Normalization の使い方\n",
    "- データ分布の正規化を行うレイヤをニューラルネットワークに挿入して使う（図6-16）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定義式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ミニバッチ\n",
    "$$\n",
    "B = {x_1, x_2, ..., x_m}\n",
    "$$\n",
    "に対して"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mu_B = \\frac{1}{m} \\sum_{i=1}^{m}{x_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m}{\\left( x_i - \\mu_B \\right)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式で行っていることは，\n",
    "- ミニバッチの入力データ ${x1, x2, .., x_m}$ を，平均0，分散1のデータ ${\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_m}$ に変換する\n",
    "\n",
    "こと．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここからさらに 正規化された ${\\hat{x}_i}$ に対して，固有のスケール $\\gamma$ とシフト $\\beta$ で変換を行う．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y_i = \\gamma \\hat{x}_i + \\beta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\gamma$, $\\beta$ はパラメータ，$\\gamma = 1, \\beta = 0$ を初期値としてこれまでの全結合層と同様に学習を行う．（$\\gamma$, $\\beta$ はスカラでない場合もある）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "微分の導出が意外と大変なんですね..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 正則化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 機械学習の問題では，過学習がよく問題になる\n",
    "- 過学習とは，訓練データだけに適応しすぎてしまい，訓練データに含まれない他のデータにはうまく対応できない状態\n",
    "- 未知のデータに対しても正しく識別できるモデルが理想\n",
    "- NN は複雑で表現力が高いモデルにすることが簡単だが，その分過学習を抑制するテクニックが重要になる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.1 過学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "過学習が起きる主な原因\n",
    "- パラメータを大量に持ち，表現力の高いモデルであること\n",
    "- 訓練データが少ないこと"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今から実際に起こしてみましょう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.2 Weight decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 昔からよく用いられているそうです\n",
    "- Weight decay: L2 ノルムを正則化項として損失関数に加算する正則化のこと\n",
    "- L1 ノルム正則化もある\n",
    "- これらを一般化した Lp ノルム正則化もある"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ノルム正則化の効果\n",
    "- L1 ノルム正則化はパラメータのスパース化\n",
    "- L2 ノルム正則化はパラメータのレンジを狭くする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際の正則化項をあわせた式は L1 正則化も含めると以下のようになる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L_{12}\\left(W_i\\right) = L\\left(W_i\\right) + \\lambda_1 \\sum_{i} \\left|W_{i}\\right| + \\frac{1}{2} \\lambda_2 \\sum_{i} W^2_{i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習の際には正則化したい重みでの微分に加えて，正則化項の微分も加算する．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.3 Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- モデルが複雑になってくると Weight Decay だけでは対応が困難になる\n",
    "- そこで Dropout を用いた過学習抑制を行う"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "何をやっているのか\n",
    "- ニューロンをランダムに消去しながら学習する手法\n",
    "- 一定の確率（これは自分で設定する）で Dropout 層に入ってきた値を伝搬させない処理を行う"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout の効果\n",
    "- 毎回異なるニューロンで推論を行うため，NN の中に冗長性が生まれ，アンサンブル学習と同じような効果が期待できる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際にその効果を動かしながら見てみましょう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 ハイパーパラメータの検証"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN におけるハイパーパラメータとは\n",
    "- 層の数\n",
    "- 各層のニューロンの数\n",
    "- バッチサイズ\n",
    "- 学習率\n",
    "- 正則化係数\n",
    "- etc.\n",
    "\n",
    "これらを適切な値に設定しなければ，学習はうまく進まない．\n",
    "しかし探索するのも一苦労．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代表的な方法\n",
    "- random search\n",
    "- grid search\n",
    "- ランダムに gird search を行う方法\n",
    "- ベイズ最適化を用いた方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}